{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the Average for Pseudo Streaming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==2.4.4 in /usr/local/lib/python3.7/dist-packages (2.4.4)\n",
      "Requirement already satisfied: py4j==0.10.7 in /usr/local/lib/python3.7/dist-packages (from pyspark==2.4.4) (0.10.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark==2.4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/workspace\n"
     ]
    }
   ],
   "source": [
    "# current location\n",
    "%cd /opt/workspace/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pyspark/jars\n"
     ]
    }
   ],
   "source": [
    "# jar location\n",
    "%cd /usr/local/lib/python3.7/dist-packages/pyspark/jars/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints\t\thw3_HsinhanHsieh.ipynb\tscala.ipynb\t test.py\n",
      "data\t\t\tkafka_streaming.ipynb\tsparkr.ipynb\n",
      "hw2_HsinhanHsieh.ipynb\tpyspark.ipynb\t\tstreaming.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.4 test.py\n",
    "# !spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8:2.4.4 test.py\n",
    "# !spark-submit --driver-memory 2000M --executor-memory 2000M --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.4 test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move jar to location\n",
    "!mv spark-sql-kafka-0-10_2.11-2.4.4.jar /usr/local/lib/python3.7/dist-packages/pyspark/jars/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/04/09 22:45:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "python: can't open file '/usr/local/lib/python3.7/dist-packages/pyspark/jars/test.py': [Errno 2] No such file or directory\n",
      "log4j:WARN No appenders could be found for logger (org.apache.spark.util.ShutdownHookManager).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    }
   ],
   "source": [
    "# use jar to run\n",
    "!spark-submit --jars spark-sql-kafka-0-10_2.11-2.4.4.jar test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.7/dist-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ba497bc0-94fe-42c6-8246-66d9e3235ef5;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.4 in central\n",
      "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
      "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
      "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
      "\tfound com.101tec#zkclient;0.3 in central\n",
      "\tfound log4j#log4j;1.2.17 in central\n",
      "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
      "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.3 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      ":: resolution report :: resolve 637ms :: artifacts dl 20ms\n",
      "\t:: modules in use:\n",
      "\tcom.101tec#zkclient;0.3 from central in [default]\n",
      "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
      "\tlog4j#log4j;1.2.17 from central in [default]\n",
      "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
      "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
      "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.4 from central in [default]\n",
      "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ba497bc0-94fe-42c6-8246-66d9e3235ef5\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/10ms)\n",
      "21/04/12 18:30:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Welcome to DataMaking !!!\n",
      "Stream Data Processing Application Started ...\n",
      "2021-04-12 18:30:59\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "21/04/12 18:30:59 INFO SparkContext: Running Spark version 2.4.4\n",
      "21/04/12 18:30:59 INFO SparkContext: Submitted application: ticket_avg\n",
      "21/04/12 18:30:59 INFO SecurityManager: Changing view acls to: root\n",
      "21/04/12 18:30:59 INFO SecurityManager: Changing modify acls to: root\n",
      "21/04/12 18:30:59 INFO SecurityManager: Changing view acls groups to: \n",
      "21/04/12 18:30:59 INFO SecurityManager: Changing modify acls groups to: \n",
      "21/04/12 18:30:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "21/04/12 18:30:59 INFO Utils: Successfully started service 'sparkDriver' on port 43261.\n",
      "21/04/12 18:30:59 INFO SparkEnv: Registering MapOutputTracker\n",
      "21/04/12 18:30:59 INFO SparkEnv: Registering BlockManagerMaster\n",
      "21/04/12 18:30:59 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "21/04/12 18:30:59 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "21/04/12 18:30:59 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-013c7569-42b1-4812-af05-cc00b6db0463\n",
      "21/04/12 18:30:59 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "21/04/12 18:31:00 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "21/04/12 18:31:00 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "21/04/12 18:31:00 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://342c44327ebb:4040\n",
      "21/04/12 18:31:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.4.jar at spark://342c44327ebb:43261/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.4.jar with timestamp 1618252260418\n",
      "21/04/12 18:31:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://342c44327ebb:43261/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1618252260419\n",
      "21/04/12 18:31:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://342c44327ebb:43261/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1618252260420\n",
      "21/04/12 18:31:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://342c44327ebb:43261/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1618252260420\n",
      "21/04/12 18:31:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://342c44327ebb:43261/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1618252260420\n",
      "21/04/12 18:31:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://342c44327ebb:43261/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1618252260420\n",
      "21/04/12 18:31:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://342c44327ebb:43261/jars/com.101tec_zkclient-0.3.jar with timestamp 1618252260420\n",
      "21/04/12 18:31:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://342c44327ebb:43261/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1618252260421\n",
      "21/04/12 18:31:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://342c44327ebb:43261/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1618252260421\n",
      "21/04/12 18:31:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://342c44327ebb:43261/jars/log4j_log4j-1.2.17.jar with timestamp 1618252260421\n",
      "21/04/12 18:31:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://342c44327ebb:43261/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1618252260421\n",
      "21/04/12 18:31:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.3.jar at spark://342c44327ebb:43261/jars/org.xerial.snappy_snappy-java-1.1.7.3.jar with timestamp 1618252260421\n",
      "21/04/12 18:31:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.4.jar at spark://342c44327ebb:43261/files/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.4.jar with timestamp 1618252260441\n",
      "21/04/12 18:31:00 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.4.jar to /tmp/spark-7787d055-abb7-46c5-becc-a5811a8ee3b4/userFiles-0579ec75-2845-43f9-b0c8-c412c67758bb/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.4.jar\n",
      "21/04/12 18:31:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://342c44327ebb:43261/files/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1618252260470\n",
      "21/04/12 18:31:00 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-7787d055-abb7-46c5-becc-a5811a8ee3b4/userFiles-0579ec75-2845-43f9-b0c8-c412c67758bb/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
      "21/04/12 18:31:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://342c44327ebb:43261/files/org.spark-project.spark_unused-1.0.0.jar with timestamp 1618252260519\n",
      "21/04/12 18:31:00 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-7787d055-abb7-46c5-becc-a5811a8ee3b4/userFiles-0579ec75-2845-43f9-b0c8-c412c67758bb/org.spark-project.spark_unused-1.0.0.jar\n",
      "21/04/12 18:31:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://342c44327ebb:43261/files/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1618252260527\n",
      "21/04/12 18:31:00 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-7787d055-abb7-46c5-becc-a5811a8ee3b4/userFiles-0579ec75-2845-43f9-b0c8-c412c67758bb/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
      "21/04/12 18:31:00 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://342c44327ebb:43261/files/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1618252260546\n",
      "21/04/12 18:31:00 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-7787d055-abb7-46c5-becc-a5811a8ee3b4/userFiles-0579ec75-2845-43f9-b0c8-c412c67758bb/com.yammer.metrics_metrics-core-2.2.0.jar\n",
      "21/04/12 18:31:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://342c44327ebb:43261/files/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1618252260556\n",
      "21/04/12 18:31:00 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-7787d055-abb7-46c5-becc-a5811a8ee3b4/userFiles-0579ec75-2845-43f9-b0c8-c412c67758bb/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
      "21/04/12 18:31:00 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://342c44327ebb:43261/files/com.101tec_zkclient-0.3.jar with timestamp 1618252260573\n",
      "21/04/12 18:31:00 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-7787d055-abb7-46c5-becc-a5811a8ee3b4/userFiles-0579ec75-2845-43f9-b0c8-c412c67758bb/com.101tec_zkclient-0.3.jar\n",
      "21/04/12 18:31:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://342c44327ebb:43261/files/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1618252260578\n",
      "21/04/12 18:31:00 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-7787d055-abb7-46c5-becc-a5811a8ee3b4/userFiles-0579ec75-2845-43f9-b0c8-c412c67758bb/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
      "21/04/12 18:31:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://342c44327ebb:43261/files/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1618252260587\n",
      "21/04/12 18:31:00 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-7787d055-abb7-46c5-becc-a5811a8ee3b4/userFiles-0579ec75-2845-43f9-b0c8-c412c67758bb/org.slf4j_slf4j-api-1.7.16.jar\n",
      "21/04/12 18:31:00 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://342c44327ebb:43261/files/log4j_log4j-1.2.17.jar with timestamp 1618252260596\n",
      "21/04/12 18:31:00 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-7787d055-abb7-46c5-becc-a5811a8ee3b4/userFiles-0579ec75-2845-43f9-b0c8-c412c67758bb/log4j_log4j-1.2.17.jar\n",
      "21/04/12 18:31:00 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://342c44327ebb:43261/files/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1618252260605\n",
      "21/04/12 18:31:00 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-7787d055-abb7-46c5-becc-a5811a8ee3b4/userFiles-0579ec75-2845-43f9-b0c8-c412c67758bb/net.jpountz.lz4_lz4-1.2.0.jar\n",
      "21/04/12 18:31:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.3.jar at spark://342c44327ebb:43261/files/org.xerial.snappy_snappy-java-1.1.7.3.jar with timestamp 1618252260615\n",
      "21/04/12 18:31:00 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.3.jar to /tmp/spark-7787d055-abb7-46c5-becc-a5811a8ee3b4/userFiles-0579ec75-2845-43f9-b0c8-c412c67758bb/org.xerial.snappy_snappy-java-1.1.7.3.jar\n",
      "21/04/12 18:31:00 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...\n",
      "21/04/12 18:31:00 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 44 ms (0 ms spent in bootstraps)\n",
      "21/04/12 18:31:02 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20210412183102-0000\n",
      "21/04/12 18:31:02 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45019.\n",
      "21/04/12 18:31:02 INFO NettyBlockTransferService: Server created on 342c44327ebb:45019\n",
      "21/04/12 18:31:02 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "21/04/12 18:31:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 342c44327ebb, 45019, None)\n",
      "21/04/12 18:31:02 INFO BlockManagerMasterEndpoint: Registering block manager 342c44327ebb:45019 with 366.3 MB RAM, BlockManagerId(driver, 342c44327ebb, 45019, None)\n",
      "21/04/12 18:31:02 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20210412183102-0000/0 on worker-20210412180155-172.18.0.4-45109 (172.18.0.4:45109) with 1 core(s)\n",
      "21/04/12 18:31:02 INFO StandaloneSchedulerBackend: Granted executor ID app-20210412183102-0000/0 on hostPort 172.18.0.4:45109 with 1 core(s), 512.0 MB RAM\n",
      "21/04/12 18:31:02 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20210412183102-0000/1 on worker-20210412180155-172.18.0.5-33299 (172.18.0.5:33299) with 1 core(s)\n",
      "21/04/12 18:31:02 INFO StandaloneSchedulerBackend: Granted executor ID app-20210412183102-0000/1 on hostPort 172.18.0.5:33299 with 1 core(s), 512.0 MB RAM\n",
      "21/04/12 18:31:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 342c44327ebb, 45019, None)\n",
      "21/04/12 18:31:02 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 342c44327ebb, 45019, None)\n",
      "21/04/12 18:31:03 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20210412183102-0000/1 is now RUNNING\n",
      "21/04/12 18:31:03 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20210412183102-0000/0 is now RUNNING\n",
      "21/04/12 18:31:06 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "21/04/12 18:31:11 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/opt/workspace/spark-warehouse').\n",
      "21/04/12 18:31:11 INFO SharedState: Warehouse path is 'file:/opt/workspace/spark-warehouse'.\n",
      "21/04/12 18:31:20 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\n",
      "21/04/12 18:31:21 INFO VerifiableProperties: Verifying properties\n",
      "21/04/12 18:31:21 INFO VerifiableProperties: Property group.id is overridden to \n",
      "21/04/12 18:31:21 INFO VerifiableProperties: Property zookeeper.connect is overridden to \n",
      "21/04/12 18:31:21 INFO SimpleConsumer: Reconnect due to socket error: java.nio.channels.ClosedChannelException\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/workspace/test.py\", line 54, in <module>\n",
      "    kafkaStream = KafkaUtils.createDirectStream(ssc, [KAFKA_TOPIC], {\"metadata.broker.list\": KAFKA_BROKERS})\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/streaming/kafka.py\", line 146, in createDirectStream\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o51.createDirectStreamWithoutMessageHandler.\n",
      ": org.apache.spark.SparkException: java.nio.channels.ClosedChannelException\n",
      "\tat org.apache.spark.streaming.kafka.KafkaCluster$$anonfun$checkErrors$1.apply(KafkaCluster.scala:387)\n",
      "\tat org.apache.spark.streaming.kafka.KafkaCluster$$anonfun$checkErrors$1.apply(KafkaCluster.scala:387)\n",
      "\tat scala.util.Either.fold(Either.scala:98)\n",
      "\tat org.apache.spark.streaming.kafka.KafkaCluster$.checkErrors(KafkaCluster.scala:386)\n",
      "\tat org.apache.spark.streaming.kafka.KafkaUtils$.getFromOffsets(KafkaUtils.scala:223)\n",
      "\tat org.apache.spark.streaming.kafka.KafkaUtilsPythonHelper.createDirectStream(KafkaUtils.scala:721)\n",
      "\tat org.apache.spark.streaming.kafka.KafkaUtilsPythonHelper.createDirectStreamWithoutMessageHandler(KafkaUtils.scala:689)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/04/12 18:31:22 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "21/04/12 18:31:22 INFO SparkUI: Stopped Spark web UI at http://342c44327ebb:4040\n",
      "21/04/12 18:31:22 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
      "21/04/12 18:31:22 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down\n",
      "21/04/12 18:31:22 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "21/04/12 18:31:22 INFO MemoryStore: MemoryStore cleared\n",
      "21/04/12 18:31:22 INFO BlockManager: BlockManager stopped\n",
      "21/04/12 18:31:22 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "21/04/12 18:31:22 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "21/04/12 18:31:22 INFO SparkContext: Successfully stopped SparkContext\n",
      "21/04/12 18:31:22 INFO ShutdownHookManager: Shutdown hook called\n",
      "21/04/12 18:31:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-d73a6aa0-f791-4b34-a9ff-29128420360e\n",
      "21/04/12 18:31:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-7787d055-abb7-46c5-becc-a5811a8ee3b4/pyspark-001c3547-1c13-4ffa-8cb4-9c3dbc52275a\n",
      "21/04/12 18:31:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-7787d055-abb7-46c5-becc-a5811a8ee3b4\n"
     ]
    }
   ],
   "source": [
    "# use package to run\n",
    "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.4 test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "an integer is required (got type bytes)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-64edda13c91b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# from pyspark import SparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# from pyspark.streaming import StreamingContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# from pyspark.streaming.kafka import KafkaUtils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/spark-2.4.7-bin-hadoop2.7/python/pyspark/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDDBarrier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkFiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/spark-2.4.7-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPy4JError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccumulators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAccumulator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBroadcast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBroadcastPickleRegistry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/spark-2.4.7-bin-hadoop2.7/python/pyspark/accumulators.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0msocketserver\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSocketServer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPickleSerializer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/spark-2.4.7-bin-hadoop2.7/python/pyspark/serializers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mxrange\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcloudpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_exception_message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/spark-2.4.7-bin-hadoop2.7/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m \u001b[0m_cell_set_template_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_cell_set_template_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/spark-2.4.7-bin-hadoop2.7/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36m_make_cell_set_template_code\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m         )\n\u001b[1;32m    125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         return types.CodeType(\n\u001b[0m\u001b[1;32m    127\u001b[0m             \u001b[0mco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_argcount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_kwonlyargcount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: an integer is required (got type bytes)"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# from pyspark import SparkContext\n",
    "# from pyspark.streaming import StreamingContext\n",
    "# from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "# from kafka import KafkaConsumer\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"ticket_avg\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"4g\").\\\n",
    "        config(\"spark.jars.packages\", \"org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.4\").\\\n",
    "        getOrCreate() \n",
    "\n",
    "# sc = SparkContext(appName=\"ticket_avg\")\n",
    "# sc.setLogLevel(\"WARN\")\n",
    "# ssc = StreamingContext(sc, 1)\n",
    "#config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:2.4.5\").\\\n",
    "#     spark.conf.set(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.4\")\n",
    "\n",
    "#     spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "KAFKA_TOPIC = 'ticket_flights_stream'\n",
    "KAFKA_BROKERS = 'localhost:9092'\n",
    "ZOOKEEPER = 'localhost:2181'\n",
    "\n",
    "sc = spark.sparkContext\n",
    "ssc = StreamingContext(sc, 1) # 2 second window\n",
    "\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "kafkaStream = KafkaUtils.createStream(ssc, ZOOKEEPER, 'consumer_group_1', {KAFKA_TOPIC:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.streaming.dstream.TransformedDStream object at 0x7f90a80ba668>\n"
     ]
    }
   ],
   "source": [
    "print(kafkaStream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_avg(x):\n",
    "    split = x.split(\",\")\n",
    "    return split[2], float(split[3])\n",
    "\n",
    "# define the function to update the inner state\n",
    "def updateFunc(new_values, running_tuple):\n",
    "    '''\n",
    "        new_values: values in current data\n",
    "        running_tuple: values in states\n",
    "    '''\n",
    "    new_sum = [field[0] for field in new_values]\n",
    "    new_count = [field[1] for field in new_values]\n",
    "    running_sum, running_count = running_tuple\n",
    "    \n",
    "    return sum(new_sum, running_sum), sum(new_count, running_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.checkpoint(\"./checkpoints\")\n",
    "initialStateRDD = sc.parallelize([(u'Economy', (0, 0)),\n",
    "                                  (u'Comfort', (0, 0)),\n",
    "                                  (u'Business', (0, 0))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = kafkaStream.map(map_avg).mapValues(lambda x: (x, 1)) \\\n",
    "        .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1])) \\\n",
    "        .updateStateByKey(updateFunc, initialRDD=initialStateRDD) \\\n",
    "        .mapValues(lambda x: (x[0] / x[1], x[0], x[1])).pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-839720b1c69e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# out.pprint()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# time.sleep(15)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/streaming/context.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mStart\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mexecution\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mstreams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \"\"\"\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0mStreamingContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activeContext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# out.pprint()\n",
    "import time\n",
    "ssc.start()\n",
    "# time.sleep(15)\n",
    "ssc.awaitTermination()\n",
    "# ssc.stop(stopSparkContext=True, stopGraceFully=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
